{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5921de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f476170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = 'Open-orca'\n",
    "filename = 'Slimorca-50'\n",
    "output = 'Slimorca-50-llm.json'\n",
    "file_path = repository + '/' + filename\n",
    "output_path = repository + '/' + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ddbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pretty-print JSON data\n",
    "def print_basic_json_structure(data):\n",
    "    pretty_json = json.dumps(data, indent=4)\n",
    "    print(pretty_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bded8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    llmmodel = 'llama'\n",
    "    try:\n",
    "        r = requests.post('http://localhost:11434/api/generate',\n",
    "                          json={\n",
    "                              'model': llmmodel,\n",
    "                              'prompt': prompt,\n",
    "                          },\n",
    "                          stream=True)\n",
    "        r.raise_for_status()\n",
    "        response_text = str(r.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during API request: {e}\")\n",
    "\n",
    "    full_response = ''\n",
    " \n",
    "    for line in r.iter_lines():\n",
    "        body = json.loads(line)\n",
    "        response_part = body.get('response', '')\n",
    "        full_response += response_part\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb28072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open-orca/Slimorca-50.json\n"
     ]
    }
   ],
   "source": [
    "# Choose a file path\n",
    "jsonl_path = file_path + '.json'\n",
    "print(jsonl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19c262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 - JSON Structure:\n",
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"Article:In 2014, 68 animals died in crashes in the national park and, while the number is falling, accident hotspots remain.\\nA Hampshire County Council report said a trial average speed zone would be discussed with police, but there was currently no funding for the scheme.\\nOther measures include the roll-out of signs with seasonal changing slogans and wild pony road markings.\\nAccording to the report being considered by councillors, an average speed check system would cost in the region of \\u00c2\\u00a3100,000 with \\u00c2\\u00a340,000 for each additional unit.\\nIt said Hampshire Constabulary warned the scheme was cost-prohibitive and needed a 3G mobile signal and electrical supply to function - a potential problem in the heart of the forest.\\nBut the report said the scheme remained an \\\"aspiration\\\" for the council.\\nThe authority's Economy, Transport and Environment Select Committee is meeting to discuss the plans.\\nCommittee member and Conservative county councillor for Ringwood, Steve Rippon-Swayne, said: \\\"It's difficult to tell exactly what more could be done [to cut animal deaths] - anything which involves enforcing speed costs money and a lot of it.\\n\\\"Money is being withdrawn by government so we have to spend wisely.\\\"\\nThe council said clusters of casualties involving ponies, cattle and other livestock occurred on three main routes - the B3078 Cadnam to Godshill, the B3054 Hatchet Pond to Portmore and the B3056 Beaulieu to Lyndhurst.\\nThe Verderers - who administer the forest - are already paying 80% of the cost towards a mobile speed camera at high-risk sites.\\nThe Hampshire County Council report said pinchpoints introduced in Brockenhurst were unlikely to be rolled out across the national park and lowering the speed limit to 30mph had also been ruled out.\\n\\nA summary of the above article is?\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"In 2014, 68 animals died in crashes in a national park in Hampshire. Despite the declining number of incidents, accident hotspots still exist. A trial average speed zone has been proposed by the Hampshire County Council, but there is currently no funding for this scheme. Other measures being considered are the introduction of signs with seasonal changing slogans and wild pony road markings. A report estimates that an average speed check system would cost around \\u00a3100,000, with an additional cost of \\u00a340,000 per unit. Hampshire Constabulary has warned that such a scheme is cost-prohibitive and would require both a 3G mobile signal and an electrical supply to function, which may be difficult in the remote forest area. The council still considers the scheme an \\\"aspiration\\\" despite these challenges. The Economy, Transport and Environment Select Committee is meeting to discuss the plans, but with decreasing government funding, enforcing speed measures will likely have a high financial cost. Currently, clusters of animal casualties involving ponies, cattle, and other livestock occur on specific routes in the area, and the Verderers, who administer the forest, already contribute towards a mobile speed camera at high-risk sites. Lowering the speed limit to 30mph has been ruled out, and pinchpoints used in Brockenhurst are also unlikely to be replicated across the national park.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Sample 2 - JSON Structure:\n",
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You are an AI assistant that follows instruction extremely well. Help as much as you can.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"Given the task definition and input, reply with output. You are given a sentence in Portuguese. Your job is to translate the Portuguese sentence into Farsi.\\n\\nE agora, quando procuramos mudar a situa\\u00e7\\u00e3o, quando procuramos uma solu\\u00e7\\u00e3o para a democracia, devemos manter em mente esta ambiguidade.\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"\\u0648 \\u062d\\u0627\\u0644\\u0627 \\u060c \\u0632\\u0645\\u0627\\u0646\\u06cc \\u06a9\\u0647 \\u0645\\u0627 \\u0628\\u0647 \\u062f\\u0646\\u0628\\u0627\\u0644 \\u062a\\u063a\\u06cc\\u06cc\\u0631 \\u0648\\u0636\\u0639\\u06cc\\u062a \\u0647\\u0633\\u062a\\u06cc\\u0645 \\u060c \\u0632\\u0645\\u0627\\u0646\\u06cc \\u06a9\\u0647 \\u0628\\u0647 \\u062f\\u0646\\u0628\\u0627\\u0644 \\u06cc\\u06a9 \\u0631\\u0627\\u0647 \\u062d\\u0644 \\u0628\\u0631\\u0627\\u06cc \\u062f\\u0645\\u0648\\u06a9\\u0631\\u0627\\u0633\\u06cc \\u0647\\u0633\\u062a\\u06cc\\u0645 \\u060c \\u0628\\u0627\\u06cc\\u062f \\u0627\\u06cc\\u0646 \\u0627\\u0628\\u0647\\u0627\\u0645 \\u0631\\u0627 \\u062f\\u0631 \\u0630\\u0647\\u0646 \\u0646\\u06af\\u0647 \\u062f\\u0627\\u0631\\u06cc\\u0645.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Sample 3 - JSON Structure:\n",
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"If A short walk west from Toshogu itself is surely the oldest of the institutions on this holy ground., can we conclude that \\\"There are no institutions to the north of Toshugu.\\\"?\\nAvailable choices:\\na). yes.\\nb). it is not possible to tell.\\nc). no.\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"b). it is not possible to tell.\\n\\nExplanation: The statement only tells us about a short walk west from Toshogu and an old institution there. It doesn't give us any information about what is to the north of Toshugu. So, we cannot conclude anything about institutions to the north of Toshugu.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Sample 4 - JSON Structure:\n",
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"Please answer the following question: On a scale of 1-5 (with 1 being least favorable and 5 being most favorable), how would you rate this review? \\\"Like it ..idk what to say but i like it.....\\\"\\nAnswer:\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"In order to rate this review on a scale of 1-5, it is crucial to take into account various aspects such as clarity, detail, helpfulness, and the opinion of the reviewer. \\n\\n1. Clarity: The reviewer has expressed that they like the product or service with the phrase \\\"Like it.\\\" However, the rest of the review (\\\"idk what to say\\\") doesn't contribute much in terms of clarity, leaving the reader wanting more information about the product or service. \\n\\n2. Detail: The review is quite vague and does not provide any specific details, insights, or opinions about the experience the reviewer had with the product or service. For example, it lacks information about the features they liked, reasons behind their liking, or any potential areas for improvement.\\n\\n3. Helpfulness: A helpful review should provide informative insights for potential customers, allowing them to make a better-informed decision about whether the product or service is right for them. This review does not fulfill that criterion, as it offers only a very general opinion without any elaboration or context. \\n\\n4. Opinion of the reviewer: Although the reviewer has stated that they \\\"like it,\\\" they fail to convey their reasons, making it difficult for readers to discern the aspects that led to their positive experience.\\n\\nTaking these factors into consideration, I would rate this review a 2 out of 5. While the reviewer did provide a positive sentiment about the product or service, the lack of clarity, detail, and helpfulness limits the review's usefulness for potential customers seeking more information. A more comprehensive and informative review would have been more beneficial for readers.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Sample 5 - JSON Structure:\n",
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"Answer the following question: Title: Timer Trippers Review: These switch trippers eliminated the need to purchase an entire timer since the only problem was damage to the original trippers by over tightening. The price was low and the shipping was prompt. Good service Does this product review convey a negative or positive sentiment?\\nAnswer:\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"Step 1: Analyze the content of the review. I will break down the key points mentioned in the review:\\n\\n1. The switch trippers eliminated the need to purchase an entire timer - this indicates that the product provided a cost-effective solution.\\n2. The only problem was damage to the original trippers by over tightening - this may be a small concern or issue.\\n3. The price was low - this indicates a positive aspect of the product.\\n4. The shipping was prompt - this indicates timely delivery, a positive aspect of the service.\\n5. Good service - this reinforces the positive sentiment regarding the service.\\n\\nStep 2: Determine the overall sentiment of the review.\\n\\nBased on the points mentioned above, I can see that the majority of the review emphasizes the positive aspects of the product and service. Only a small issue is mentioned regarding the original trippers, but this is outweighed by the numerous positive comments.\\n\\nConclusion: This product review conveys a positive sentiment.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Open the JSONL file for reading\n",
    "with open(jsonl_path, 'r') as jsonl_file:\n",
    "    # Read all lines from the JSONL file\n",
    "    lines = jsonl_file.readlines()\n",
    "        \n",
    "# Ensure that there are at least 10 lines in the file\n",
    "if len(lines) < 5:\n",
    "    print(\"There are not enough lines in the file to sample 5.\")\n",
    "else:\n",
    "    # Randomly sample and print 10 JSON structures\n",
    "    sampled_lines = random.sample(lines, 5)\n",
    "    for i, line in enumerate(sampled_lines):\n",
    "        sampled_lines = random.sample(lines, 5)\n",
    "        json_data = json.loads(line)\n",
    "        print(f\"Sample {i + 1} - JSON Structure:\")\n",
    "        print_basic_json_structure(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c026550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in the file 'Open-orca/Slimorca-50.json' is: 258991\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of lines\n",
    "num_lines = len(lines)\n",
    "\n",
    "# Print the number of lines\n",
    "print(f\"The number of lines in the file '{jsonl_path}' is: {num_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8140345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I want you to act as a writing assistant than understands the linguistics patterns, tone, and messaging of the Rebel brand archetype. Your job is to rewrtie this text, you should not talk about what you did, if faced with a question do not answer the question. Only respond with the messaging of the Rebel brand archetype. No emojis, Respond in exactly \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba4369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Timestamp: 2023-12-19 19:06:06.924889\n",
      "Line number : 0\n",
      "Line number : 1\n",
      "Line number : 2\n",
      "Line number : 3\n",
      "Line number : 4\n",
      "Line number : 5\n",
      "Line number : 6\n",
      "Line number : 7\n",
      "Line number : 8\n",
      "Line number : 9\n",
      "Line number : 10\n",
      "Line number : 11\n",
      "Line number : 12\n",
      "Line number : 13\n",
      "Line number : 14\n",
      "Line number : 15\n",
      "Line number : 16\n",
      "Line number : 17\n",
      "Line number : 18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m length_of_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(response)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(\"Before LLM --------------------------------  : \\n\" + response)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Generate LLM Response \u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlength_of_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcharacters. \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(\"After LLM -------------------------------- : \\n\" + summary)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Add the summary as 'newvalue'\u001b[39;00m\n\u001b[1;32m     26\u001b[0m conversation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m summary\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      4\u001b[0m     r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://localhost:11434/api/generate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                       json\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      6\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: llmmodel,\n\u001b[1;32m      7\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt,\n\u001b[1;32m      8\u001b[0m                       },\n\u001b[1;32m      9\u001b[0m                       stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m---> 11\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during API request: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:923\u001b[0m, in \u001b[0;36mResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    921\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding\n\u001b[0;32m--> 923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Fallback to auto-detected encoding.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:931\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:1071\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1071\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:999\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# all lines code\n",
    "# all lines code\n",
    "# Get the current date and time\n",
    "current_time = datetime.datetime.now()\n",
    "# Print the timestamp\n",
    "print(\"Starting Timestamp:\", current_time)\n",
    "\n",
    "with open(jsonl_path, 'r') as jsonl_file, open(output_path, 'w') as output_jsonl_file:\n",
    "    gpt_response_count = 0\n",
    "    with open(output_path, 'w') as output_jsonl_file:\n",
    "        for i, line in enumerate(lines):\n",
    "            json_data = json.loads(line)\n",
    "            \n",
    "            # Print only GPT responses\n",
    "            for conversation in json_data.get(\"conversations\", []):\n",
    "                if conversation.get(\"from\") == \"gpt\":\n",
    "                    print(f\"Line number : {gpt_response_count}\")\n",
    "                    # Retrieve the response text from the 'value' key of the conversation.\n",
    "                    response = conversation.get(\"value\")\n",
    "                    length_of_response = len(response)\n",
    "                    # print(\"Before LLM --------------------------------  : \\n\" + response)\n",
    "                    # Generate LLM Response \n",
    "                    summary = generate(prompt + str(length_of_response) + \"characters. \" + response)\n",
    "                    # print(\"After LLM -------------------------------- : \\n\" + summary)\n",
    "                    # Add the summary as 'newvalue'\n",
    "                    conversation[\"newvalue\"] = summary\n",
    "                    gpt_response_count += 1\n",
    "            \n",
    "            # Convert the modified json_data back to a JSON string\n",
    "            modified_line = json.dumps(json_data)\n",
    "\n",
    "            # Write the modified JSON string to the new file\n",
    "            output_jsonl_file.write(modified_line + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8cbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "current_time = datetime.datetime.now()\n",
    "# Print the timestamp\n",
    "print(\"Ending Timestamp:\", current_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
